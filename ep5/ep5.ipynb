{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2K5E2227Ff3",
        "outputId": "e7e4eb1b-135e-494c-84f3-ee44ac150b21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:                    x86_64\n",
            "CPU op-mode(s):                  32-bit, 64-bit\n",
            "Byte Order:                      Little Endian\n",
            "Address sizes:                   46 bits physical, 48 bits virtual\n",
            "CPU(s):                          2\n",
            "On-line CPU(s) list:             0,1\n",
            "Thread(s) per core:              2\n",
            "Core(s) per socket:              1\n",
            "Socket(s):                       1\n",
            "NUMA node(s):                    1\n",
            "Vendor ID:                       GenuineIntel\n",
            "CPU family:                      6\n",
            "Model:                           85\n",
            "Model name:                      Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "Stepping:                        3\n",
            "CPU MHz:                         2000.140\n",
            "BogoMIPS:                        4000.28\n",
            "Hypervisor vendor:               KVM\n",
            "Virtualization type:             full\n",
            "L1d cache:                       32 KiB\n",
            "L1i cache:                       32 KiB\n",
            "L2 cache:                        1 MiB\n",
            "L3 cache:                        38.5 MiB\n",
            "NUMA node0 CPU(s):               0,1\n",
            "Vulnerability Itlb multihit:     Not affected\n",
            "Vulnerability L1tf:              Mitigation; PTE Inversion\n",
            "Vulnerability Mds:               Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:          Vulnerable\n",
            "Vulnerability Mmio stale data:   Vulnerable\n",
            "Vulnerability Retbleed:          Vulnerable\n",
            "Vulnerability Spec store bypass: Vulnerable\n",
            "Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and use\n",
            "                                 rcopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PB\n",
            "                                 RSB-eIBRS: Not affected\n",
            "Vulnerability Srbds:             Not affected\n",
            "Vulnerability Tsx async abort:   Vulnerable\n",
            "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n",
            "                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n",
            "                                 se2 ss ht syscall nx pdpe1gb rdtscp lm constant\n",
            "                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid \n",
            "                                 tsc_known_freq pni pclmulqdq ssse3 fma cx16 pci\n",
            "                                 d sse4_1 sse4_2 x2apic movbe popcnt aes xsave a\n",
            "                                 vx f16c rdrand hypervisor lahf_lm abm 3dnowpref\n",
            "                                 etch invpcid_single ssbd ibrs ibpb stibp fsgsba\n",
            "                                 se tsc_adjust bmi1 hle avx2 smep bmi2 erms invp\n",
            "                                 cid rtm mpx avx512f avx512dq rdseed adx smap cl\n",
            "                                 flushopt clwb avx512cd avx512bw avx512vl xsaveo\n",
            "                                 pt xsavec xgetbv1 xsaves arat md_clear arch_cap\n",
            "                                 abilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVrfD6pW6NSe",
        "outputId": "544f7824-538c-4c17-c0e7-bc858285f872"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 14 13:29:45 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    32W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SY1sxGa5lSO",
        "outputId": "3ba64001-9c0d-4362-df93-aac4f39697f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_multiply_2d.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matrix_multiply_2d.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define N 256\n",
        "\n",
        "__global__ void matrixMulGPU( int * a, int * b, int * c )\n",
        "{\n",
        "    int ROW = blockIdx.y*blockDim.y+threadIdx.y;\n",
        "    int COL = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "\n",
        "    int val = 0;\n",
        "\n",
        "    if (ROW < N && COL < N) {\n",
        "        // each thread computes one element of the block sub-matrix\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            val += a[ROW * N + i] * b[i * N + COL];\n",
        "            //printf(\"val: %d\\n\", val);\n",
        "        }\n",
        "    }\n",
        "    c[ROW * N + COL] = val;\n",
        "}\n",
        "\n",
        "/*\n",
        " * This CPU function already works, and will run to create a solution matrix\n",
        " * against which to verify your work building out the matrixMulGPU kernel.\n",
        " */\n",
        "\n",
        "void matrixMulCPU( int * a, int * b, int * c )\n",
        "{\n",
        "  int val = 0;\n",
        "\n",
        "  for( int row = 0; row < N; ++row )\n",
        "    for( int col = 0; col < N; ++col )\n",
        "    {\n",
        "      val = 0;\n",
        "      for ( int k = 0; k < N; ++k )\n",
        "        val += a[row * N + k] * b[k * N + col];\n",
        "      c[row * N + col] = val;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int *a, *b, *c_cpu, *c_gpu; // Allocate a solution matrix for both the CPU and the GPU operations\n",
        "\n",
        "  int size = N * N * sizeof (int); // Number of bytes of an N x N matrix\n",
        "\n",
        "  struct timeval time_start;\n",
        "  struct timeval time_end;\n",
        "\n",
        "  // Allocate memory\n",
        "  cudaMallocManaged (&a, size);\n",
        "  cudaMallocManaged (&b, size);\n",
        "  cudaMallocManaged (&c_cpu, size);\n",
        "  cudaMallocManaged (&c_gpu, size);\n",
        "\n",
        "  // Initialize memory; create 2D matrices\n",
        "  for( int row = 0; row < N; ++row )\n",
        "    for( int col = 0; col < N; ++col )\n",
        "    {\n",
        "      a[row*N + col] = row;\n",
        "      b[row*N + col] = col+2;\n",
        "      c_cpu[row*N + col] = 0;\n",
        "      c_gpu[row*N + col] = 0;\n",
        "    }\n",
        "\n",
        "  /*\n",
        "   * Assign `threads_per_block` and `number_of_blocks` 2D values\n",
        "   * that can be used in matrixMulGPU above.\n",
        "   */\n",
        "\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  dim3 threads_per_block(16,16,1);\n",
        "  dim3 number_of_blocks(16,16,1);\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  matrixMulGPU <<< number_of_blocks, threads_per_block >>> ( a, b, c_gpu );\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  float milliseconds = 0;\n",
        "  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "  // Call the CPU version to check our work\n",
        "  gettimeofday(&time_start, NULL);\n",
        "\n",
        "  matrixMulCPU( a, b, c_cpu );\n",
        "\n",
        "  gettimeofday(&time_end, NULL);\n",
        "\n",
        "  double exec_time = (double) (time_end.tv_sec - time_start.tv_sec) +\n",
        "                       (double) (time_end.tv_usec - time_start.tv_usec) / 1000000.0;\n",
        "\n",
        "  // Compare the two answers to make sure they are equal\n",
        "  bool error = false;\n",
        "  for( int row = 0; row < N && !error; ++row )\n",
        "    for( int col = 0; col < N && !error; ++col )\n",
        "      if (c_cpu[row * N + col] != c_gpu[row * N + col])\n",
        "      {\n",
        "        printf(\"FOUND ERROR at c[%d][%d]\\n\", row, col);\n",
        "        printf(\"CPU = %d / GPU = %d\\n\", c_cpu[row * N + col], c_gpu[row * N + col]);\n",
        "        error = true;\n",
        "        break;\n",
        "      }\n",
        "  if (!error){\n",
        "    printf(\"Success!\\n\");\n",
        "    printf(\"Tempo de execução na GPU = %10.8f \\n \",milliseconds/1000);\n",
        "    printf(\"Tempo de execução na CPU = %10.8f \\n \",exec_time);\n",
        "  }\n",
        "\n",
        "  // Free all our allocated memory\n",
        "  cudaFree(a); cudaFree(b);\n",
        "  cudaFree( c_cpu ); cudaFree( c_gpu );\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o matrix_multiply_2d.cu matrix_multiply_2d.cu -run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBvqRjjMHeYD",
        "outputId": "82a5d299-9331-42b3-bcab-0f6611bfeb22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "Tempo de execução na GPU = 0.00093069 \n",
            " Tempo de execução na CPU = 0.05019100 \n",
            " "
          ]
        }
      ]
    }
  ]
}